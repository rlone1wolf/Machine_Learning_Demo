{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(fname, path='../99_image/', size=(20,20)):\n",
    "    \"\"\"读取*.png图片并显示\"\"\"\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(plt.imread(path+fname))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_describe(data):\n",
    "    \"\"\"输出数据的基本信息\"\"\"\n",
    "    _mean = 0.0 if data.mean() < 1e-11 else  data.mean()\n",
    "    _std = data.std()\n",
    "    _min = data.min()\n",
    "    _max = data.max()\n",
    "    _norm_1 = np.linalg.norm(data, ord=1)\n",
    "    _norm_2 = np.linalg.norm(data, ord=2)\n",
    "    if 1-_norm_2 < 1e-11: _norm_2 = 1.0\n",
    "    print('mean={0}, std={1},'.format(_mean, _std,))\n",
    "    print('min={0}, max={1},'.format(_min, _max,))\n",
    "    print('l1_norm={0}, l2_norm={1}'.format(_norm_1, _norm_2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以怎样发现一个数据集的底层结构？我们可以怎样最有用地对其进行归纳和分组？我们可以怎样以一种压缩格式有效地表征数据？这都是无监督学习的目标，之所以称之为「无监督」，是因为这是从无标签的数据开始学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在这里探索的两种无监督学习任务是：   \n",
    "* 将数据按相似度聚类（clustering）成不同的分组；(K 均值聚类、层次聚类)\n",
    "* 降维（reducing dimensionality）以便在保留数据结构和有用性的同时对数据进行压缩。(主成分分析（PCA）)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#导入IRIS数据集\n",
    "iris = load_iris()\n",
    "#特征矩阵\n",
    "print(iris.data[:5])\n",
    "#目标向量\n",
    "print(iris.target[:5])\n",
    "#特征名称\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_number = pd.DataFrame(iris.data, \n",
    "                           columns=iris.feature_names, )\n",
    "iris_number.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 聚类\n",
    "聚类的目标是为数据点分组，使得不同聚类中的数据点是不相似的，同一聚类中的数据点则是类似的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 K 均值聚类\n",
    "使用 K 均值聚类，我们希望将我们的数据点聚类为 K 组。K 更大时，创造的分组就更小，就有更多粒度；K 更小时，则分组就更大，粒度更少。\n",
    "\n",
    "该算法的输出是一组「标签」，这些标签将每个数据点都分配到了 K 组中的一组。在 K 均值聚类中，这些组的定义方式是为每个组创造一个重心（centroid）。这些重心就像是聚类的心脏，它们可以「捕获」离自己最近的点并将其加入到自己的聚类中。\n",
    "\n",
    "你可以把这些重心看作是派对上成为关注焦点的人，他们就像是有磁性一样。如果只有一个这样的人，每个人都会围绕在他周围；如果有很多这样的人，就会形成很多更小一点的活动中心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K 均值聚类的步骤如下：\n",
    "\n",
    "* 定义 K 个重心。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）\n",
    "\n",
    "* 寻找最近的重心并且更新聚类分配。将每个数据点都分配给这 K 个聚类中的一个。每个数据点都被分配给离它们最近的重心的聚类。这里的「接近程度」的度量是一个超参数——通常是欧几里得距离（Euclidean distance）。\n",
    "\n",
    "* 将重心移动到它们的聚类的中心。每个聚类的重心的新位置是通过计算该聚类中所有数据点的平均位置得到的。\n",
    "\n",
    "重复第 2 和 3 步，直到每次迭代时重心的位置不再显著变化（即直到该算法收敛）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 1, 1, 5, 5, 1, 5, 1, 1, 5, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       1, 5, 1, 1, 5, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 1, 1, 5, 5, 1, 1, 5,\n",
       "       5, 1, 5, 1, 5, 1, 0, 0, 0, 3, 0, 0, 0, 3, 0, 3, 3, 0, 3, 0, 3, 0,\n",
       "       0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0,\n",
       "       3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 4, 0, 2, 4, 4, 2, 3, 2, 4, 2,\n",
       "       4, 4, 4, 0, 4, 4, 4, 2, 2, 0, 4, 0, 2, 0, 4, 2, 0, 0, 4, 2, 2, 2,\n",
       "       4, 0, 4, 2, 4, 4, 0, 4, 4, 4, 0, 4, 4, 4, 0, 4, 4, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "#K-means,在训练集上训练\n",
    "mb_kmeans = MiniBatchKMeans(n_clusters = 6)\n",
    "mb_kmeans.fit(iris_number)\n",
    "\n",
    "# 在训练集和测试集上测试\n",
    "y_val_pred = mb_kmeans.predict(iris_number)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.366230896649191"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K值的评估标准\n",
    "#常见的方法有轮廓系数Silhouette Coefficient和Calinski-Harabasz Index\n",
    "#这两个分数值越大则聚类效果越好\n",
    "#CH_score = metrics.calinski_harabaz_score(X_train,mb_kmeans.predict(X_train))\n",
    "metrics.silhouette_score(iris_number, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2  层次聚类\n",
    "层次聚类类似于常规的聚类，只是你的目标是构建一个聚类的层次。如果你最终的聚类数量不确定，那这种方法会非常有用。比如说，假设要给 Etsy 或亚马逊等网络市场上的项目分组。在主页上，你只需要少量大组方便导航，但随着你的分类越来越特定，你需要的粒度水平也越来越大，即区别更加明显的项聚类。\n",
    "\n",
    "在算法的输出方面，除了聚类分配，你也需要构建一个很好的树结构，以帮助你了解这些聚类之间的层次结构。然后你可以从这个树中选择你希望得到的聚类数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层次聚类的步骤如下：\n",
    "\n",
    "* 首先从 N 个聚类开始，每个数据点一个聚类。\n",
    "\n",
    "* 将彼此靠得最近的两个聚类融合为一个。现在你有 N-1 个聚类。\n",
    "\n",
    "* 重新计算这些聚类之间的距离。有很多可以办到这件事的方法（参见这个教程了解更多细节：https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html）。其中一种方法（平均连接聚类，average-linkage clustering）是将两个聚类之间的距离看作是它们各自元素之间所有距离的平均。\n",
    "\n",
    "* 重复第 2 和 3 步，直到你得到包含 N 个数据点的一个聚类。你就会得到如下图所示的树（也被称为树状图））。\n",
    "\n",
    "* 选择一个聚类数量，然后在这个树状图中划一条水平线。比如说，如果你想要 K=2 个聚类，你应该在距离大约为 20000 的位置画一条水平线，你会得到一个包含数据点 8、9、11、16 的聚类和包含其它数据点的另一个聚类。一般而言，你得到的聚类的数量就是水平线与树状图中的竖直线的交叉点的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
       "        0, -1,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
       "        0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  1,\n",
       "       -1,  1,  4,  2, -1, -1, -1,  2, -1, -1, -1, -1,  3, -1,  2, -1,  4,\n",
       "       -1,  4,  5, -1, -1,  3,  2,  2, -1, -1,  3, -1,  4,  4,  4, -1, -1,\n",
       "       -1,  1, -1,  4,  4,  4,  3,  4, -1,  4,  4,  4,  2, -1,  4, -1,  6,\n",
       "       -1,  7, -1, -1, -1, -1, -1, -1, -1, -1, -1,  6, -1, -1,  7, -1, -1,\n",
       "       -1,  8, -1, -1,  5,  8, -1,  5,  5, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  7,  5, -1,  8, -1,  6,  8,  8, -1,  5, -1, -1,  5],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "#DBSCAN,在训练集上训练\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=3)\n",
    "y_train_pred = dbscan.fit_predict(iris_number)\n",
    "\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03189278933444991"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K值的评估标准\n",
    "#常见的方法有轮廓系数Silhouette Coefficient和Calinski-Harabasz Index\n",
    "#这两个分数值越大则聚类效果越好\n",
    "#CH_score = metrics.calinski_harabaz_score(X_train,mb_kmeans.predict(X_train))\n",
    "metrics.silhouette_score(iris_number, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 3.5 1.4 0.2]\n",
      "[5.1 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(X[0,:])\n",
    "print(X_new[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 降维\n",
    "降维看上去很像压缩。这是为了在尽可能保存相关的结构的同时降低数据的复杂度。如果你有一张简单的 128×128×3 像素的图像（长×宽×RGB 值），那么数据就有 49152 维。如果你可以给这个图像空间降维，同时又不毁掉图像中太多有意义的内容，那么你就很好地执行了降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 主成分分析（PCA）\n",
    "\n",
    "我们可以修改空间的基础。现在想象有更高维度的空间，比如有 5 万维。你可以为这个空间选择一个基础，然后根据这个基础仅选择 200 个最重要的向量。这些基向量被称为主成分，而且你可以选择其中一个子集构成一个新空间，它的维度比原来的空间少，但又保留了尽可能多的数据复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要选择出最重要的主成分，我们需要检查这些数据的方差，并按这个指标给它们排序。\n",
    "\n",
    "理解 PCA 的另一个思路是 PCA 将我们数据中存在的空间重映射成了一个更加紧凑的空间。这种变换后的维度比原来的维度更小。\n",
    "\n",
    "仅需使用重映射空间的前几个维度，我们就可以开始理解这个数据集的组织结构。这就是降维的目的：减少复杂度（即这里的维度），同时保留结构（方差）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.68420713,  0.32660731],\n",
       "       [-2.71539062, -0.16955685],\n",
       "       [-2.88981954, -0.13734561],\n",
       "       [-2.7464372 , -0.31112432],\n",
       "       [-2.72859298,  0.33392456]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 保留95%的信息\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(iris_number)\n",
    "\n",
    "# 在训练集和测试集降维 \n",
    "iris_number_pca = pca.transform(iris_number)\n",
    "iris_number_pca[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris_number由原来的4维降为了2维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 奇异值分解（SVD）\n",
    "对矩阵进行分解, 可以分解为三个矩阵, 也可以分解为二个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 树模型聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Default XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "y = iris.target\n",
    "dtrain = xgb.DMatrix(iris_number, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:36:35] C:\\dev\\libs\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:36:35] C:\\dev\\libs\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:36:35] C:\\dev\\libs\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:36:35] C:\\dev\\libs\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth':3,}\n",
    "n_trees = 4\n",
    "xgb1 = xgb.train(params, dtrain, n_trees,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [5, 5, 5, 5],\n",
       "       [5, 5, 5, 5],\n",
       "       [4, 6, 6, 6],\n",
       "       [5, 5, 5, 5],\n",
       "       [5, 5, 5, 5],\n",
       "       [4, 8, 8, 4],\n",
       "       [4, 8, 8, 4],\n",
       "       [4, 8, 8, 4],\n",
       "       [4, 8, 8, 4],\n",
       "       [4, 8, 8, 4]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_leaf = xgb1.predict(dtrain, pred_leaf=True)\n",
    "pred_leaf[::10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4棵树, 各自有各自的叶子节点索引的输出   \n",
    "叶子节点索引的最大值为$2^3=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
